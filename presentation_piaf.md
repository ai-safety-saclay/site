Notre projet, c'est le P.I.A.F.

P.I.A.F est un groupe de travail autour de la sécurité et la fiabilité des systèmes d'intelligence artificielle. Il est entièrement géré par des étudiants de [Paris-Saclay](https://fr.wikipedia.org/wiki/Paris-Saclay).

P.I.A.F signifie « Pour une Intelligence Artificielle Fiable ».

# Activités

Notre groupe a deux activités principales :


## Sensibiliser et changer le discours autour de la sûreté de l'IA

- Participer à la création de conférences (voir https://teletalks.fr/projects#asimov)
- Motiver les étudiants à intégrer la sûreté de l'IA dans leur carrière
- Avoir autant que possible un impact médiatique

## Démocratiser le domaine de recherche de la sûreté de l'IA

- Proposer des groupes de lecture, des hackathons
- Lancer des projets sur la sécurité de l'IA au sein des écoles
- Aider autant que possible dans la création et le développement de startups d'audit des systèmes d'IA



# Raison d'être

Jusqu'à il y a quelques années, l'IA était un outil qui nous assistait dans nos prises de décision. Ce n'est plus seulement le cas, et la tendance est très claire : dans le futur, les IA prendront énormément de décisions par elles-mêmes. Des IA feront toujours plus de recommandations sur les réseaux sociaux, guideront des juristes dans leurs décisions, participeront au processus de recrutement, organiseront nos agendas et enverront nos mails, seront responsables d'infrastructures de grande échelle, et seront prédominantes sur le web.

Notre mission n'est pas de vous dire si c'est une bonne ou une mauvaise chose. Ce qui est sûr, c'est que que les solutions techniques pour contrôler les IA et faire en sorte qu'elles respectent nos valeurs morales ne sont pas à la hauteur. Nous souhaitons que les étudiants aient une meilleure vision des challenges techniques et sociaux autour de l'IA : la **robustesse**, l'**alignement**, le **contrôle**.

Nous ne sommes pas militants pour une cause particulière. Notre but, c'est que les étudiants s'emparent de ces problèmes sociétaux et techniques, qu'ils participent au développement du domaine de la sûreté de l'IA et qu'ils puissent se faire leur propre idée des risques potentiels.

# Nous aider

Nous sommes véritablement passionnés par le sujet de la fiabilité des IA, mais nous avons besoin de vous pour avoir le plus d'impact possible.

Vous pouvez d'abord partager l'intiative, et en parler à tous les étudiants du plateau de Saclay susceptibles d'être intéressés.

Vous pouvez venir discuter avec nous, pour approfondir le sujet et participer à nos groupes de lecture.
