---
title: Comment ajouter un filigrane (watermark) √† du texte g√©n√©r√© par IA
---

*pr√©requis* : bases en probabilit√©s, softmax et notion de "token" pour un LLM.

Cet article est bas√© sur [ce papier](https://arxiv.org/pdf/2301.10226#cite.grinbaum_ethical_2022)
# But

Un LLM est un mod√®le entra√Æn√© sp√©cifiquement pour √©crire le texte le plus probable, ou en tout cas le plus attendu par l'utilisateur, √† partir d'un prompt.

Malheureusement, les LLM sont utilis√©s massivement pour diff√©rentes t√¢ches qui menacent le bien commun, comme:
- la r√©daction de devoirs sans par les √©l√®ves sans aucune r√©flexion
- l'√©criture de mails de phishing
- l'√©criture de posts sur les m√©dias sociaux pour encourager la m√©sinformation

Dans ces contextes, il est important de pouvoir v√©rifier qu'un texte a √©t√© √©crit ou non par une machine. Pour cela, on modifie le comportement du mod√®le en faisant en sorte que la modification soit imperceptible pour un humain. C'est ce qu'on appelle le *watermarking* (filigrane en fran√ßais).
# Strat√©gies

Il existe diff√©rentes approches pour attaquer ce probl√®me. Chaque approche est un compromis entre ces deux extr√™mes:

**Modification mineure**

La distribution de probabilit√© conditionnelle de chaque token est peu modifi√©e. On obtient des textes tr√®s coh√©rents, surtout pour les textes tr√®s pr√©dictibles ("Ma√Ætre corbeau sur un arbre perch√© ..."). Cependant, il faut un texte tr√®s long pour identifier si il a √©t√© √©crit par un LLM ou non.

**Modification majeure**

La distribution de probabilit√© conditionnelle de chaque token est beaucoup modifi√©e. On peut identifier si un texte a √©t√© √©crit par un LLM au bout de quelques mots, mais les textes sont moins coh√©rents (surtout pour les textes tr√®s pr√©dictibles, qui seront modifi√©s et donc rep√©r√©s par l'humain).
## Red-words

Cette approche est tr√®s simple: on associe √† chaque token une liste de tokens autoris√©s (verts) et une liste de tokens interdits (rouges) choisis al√©atoirement.

Pour g√©n√©rer un texte, on regarde le dernier token (en position *$i$) et on g√©n√®re le token le plus probable parmi les tokens autoris√©s pour $t_i$.


> [!NOTE]
> Cela revient √† interdire la moiti√© des digrammes, c'est √† dire des suites de tokens de longueur 1


On suppose d'abord qu'il y a autant de tokens rouges que de tokens verts, distribu√©s de mani√®re uniforme.

Supposons que l'on g√©n√®re un texte de $T$ tokens √† l'aide de ce mod√®le.

Un d√©tecteur regarde le texte en notant √† chaque fois si un token est suivi d'un autre dans la liste verte ou dans la liste rouge. Si tous ou presque sont verts, il peut conclure que le texte est g√©n√©r√© par le mod√®le. Si la moiti√© est verte et la moiti√© est rouge, il peut conclure que le texte est g√©n√©r√© par un humain.

La probabilit√© de se tromper avec ce genre de d√©tecteur est de l'ordre de $1/2^T$


Mais certains encha√Ænements de mots sont tout simplement interdits: par exemple, "camion" pourrait avoir comme mot rouge "poubelle", et donc le mod√®le serait incapable de g√©n√©rer un texte sur les camion-poubelles.


## Meilleur choix des mots interdits

Pour am√©liorer l'approche pr√©c√©dente, on pourrait ne choisir un mot vert que si il est plus probable qu'un certain seuil.

Par exemple, dans la phrase "Le pr√©sident de la France est Emmanuel ...", le mot "Macron" est extr√™mement probable. Si le mot "Macron" est rouge, on va alors choisir le mot vert le plus probable (disons "Kant"), ce qui n'est pas du tout coh√©rent.

Cette approche a l'avantage de ne pas modifier ou presque les texte pr√©dictibles (dits de faible **entropie**).

Mais cette approche est difficile √† analyser math√©matiquement, le choix du seuil n'est pas facile.

Une approche plus satisfaisante math√©matiquement est d√©crite dans la partie suivante

## Soft-choice

Dans cette approche, on va rendre moins probable tous les digrammes rouges, et rendre moins probable tous les digrammes verts. On veut la propri√©t√© suivante: si $X$ est tr√®s probablement suivi par $Y$ mais que $XY$ est rouge, $X$ reste probablement suivi par $Y$.

Une mani√®re de garantir cela est d'utiliser une fonction "softmax".

√âtant donn√© tous les tokens pr√©c√©dents, on peut math√©matiquement attribuer √† chaque token $X$ un score $l(X)$ tel que la distribution de probabilit√© propos√©e pour le prochain token est $p(X) = \exp(-l(Y))/ \sum_{j}\exp (-l(Y))$

On va ajouter une p√©nalit√© pour tous les mots rouges (autrement dit un bonus pour tous les mots verts)

Notons $S$ le token pr√©c√©dent dans le texte. On pose  $l'(X) = l(X)$  si $SX$ est rouge et $l'(X) = l(X)+\delta$ si $SX$ est vert.

On a alors $p'(X) = \exp(-l'(X))/ \sum_{j}\exp (-l'(Y))$

Distribution originale:

![](Drawing 2024-09-19 22.07.07.excalidraw.svg)
%%[[Drawing 2024-09-19 22.07.07.excalidraw.md|üñã Edit in Excalidraw]]%%

Distribution modifi√©e:

![](watermarking-llm 2024-09-19 22.11.07.excalidraw.svg)
%%[[watermarking-llm 2024-09-19 22.11.07.excalidraw.md|üñã Edit in Excalidraw]]%%

## Formalisation


Soit $PROMPT=s_{-N}...s_{-1}$ les tokens de la question pos√©e au LLM.

On note $S=s_{0}...s_{T-1}$ la variable al√©atoire correspondant √† la r√©ponse au prompt par le LLM sans watermark, et $W$ sa r√©ponse avec watermark (de longueur $T$)

Enfin, on note $G$ le nombre de digrammes verts dans $S$

Pour d√©cider si une compl√©tion a √©t√© faite par un LLM, on regarde si $G$ est sup√©rieur √† un seuil. Pour d√©terminer ce seuil, on utilise l'in√©galit√© de Bienaym√©-Tchebychev.

$P(G - \mathbb E(G) > \alpha)\ \leq \dfrac{Var(G)}{\alpha^2}$ 

Il faut alors d√©terminer la variance et l‚Äôesp√©rance de $G$.

Celles-ci vont d√©pendre du prompt. Par exemple, si le prompt est "√©crit 100x le mot bonjour", il y a une seule r√©ponse probable. Si le digramme "bonjour bonjour" est vert, alors $G=100$. Sinon, $G=0$

Une propri√©t√© souhaitable est $H(W|PROMPT) \approx H(S|PROMPT)$: on veut que la compl√©tion avec watermark ne soit pas surprenante (ou en tout cas pas trop surprenante) que celle sans watermark.

Il y a donc un compromis entre la sensitivit√© du d√©tecteur et la qualit√© de la r√©ponse souhait√©e. On a pu mesurer ce compromis sur un jeu de donn√©es (en haut plus de sensitivit√©, √† droite meilleure qualit√©):
![](Pasted image 20240919224629.png)

Empiriquement, on a montr√© que l'on peut choisir un seuil ind√©pendant du prompt qui permet d'identifier si une suite de 128 tokens est g√©n√©r√©e par le LLM avec watermark 98% du temps. Les 2% d'erreurs sont caus√©e par des prompts dont les r√©ponses sont presque certaines (entropie faible).


# Limites

On peut lister 2 vecteurs d'attaque majeurs:

## L'attaque par instructions et effacement

Dans cette attaque, on demande par exemple au LLM "Ajoute un emoji un mot sur deux". Ensuite, il suffit de supprimer tous les emojis g√©n√©r√©s, pour obtenir un r√©sultat qui n'a aucune chance d'√™tre watermarked.

Ce vecteur d'attaque peut √™tre √©vit√© en interdisant au LLM d'effectuer ce genre d'actions, mais savoir comment forcer un LLM √† ne pas avoir un comportement est encore tr√®s difficile (voir [[RLHF]]).


## L'attaque par reformulation

Dans cette attaque, on g√©n√®re un texte watermarqu√© avec un LLM, et on utilise un autre LLM pour reformuler l'information.

√âtant donn√© que beaucoup de mod√®les sont open-source, ceci peut se faire tr√®s facilement. D'autant plus que reformuler est une t√¢che tr√®s simple pour un LLM, et on perd peu de qualit√© d'information.

C'est toute la difficult√© associ√©e aux mod√®les open-source.